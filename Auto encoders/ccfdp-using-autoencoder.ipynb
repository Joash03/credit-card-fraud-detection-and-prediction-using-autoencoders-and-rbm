{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Detection and Prediction Using Autoencoder\nThis project demonstrates the use of unsupervised training for Credit Card Fraud Detection and Prediction. The main model implemented here is Autoencoder, which achives 0.96 of AUC on test set.","metadata":{}},{"cell_type":"markdown","source":"# Load libraries ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \nimport os\nfrom datetime import datetime \nimport seaborn as sns\n\nfrom pylab import rcParams\nimport sklearn\nfrom sklearn.metrics import roc_auc_score as auc \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)\n\nrcParams['figure.figsize'] = 10, 10\nLABELS = [\"Normal\", \"Fraud\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:01.870422Z","iopub.execute_input":"2021-06-09T08:51:01.871132Z","iopub.status.idle":"2021-06-09T08:51:09.40186Z","shell.execute_reply.started":"2021-06-09T08:51:01.870989Z","shell.execute_reply":"2021-06-09T08:51:09.400064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration\n\nThe dataset we're going to use can be downloaded from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud).The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","metadata":{}},{"cell_type":"code","source":"data_fdp = pd.read_csv('../input/creditcardfraud/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:09.4045Z","iopub.execute_input":"2021-06-09T08:51:09.40502Z","iopub.status.idle":"2021-06-09T08:51:15.15734Z","shell.execute_reply.started":"2021-06-09T08:51:09.404954Z","shell.execute_reply":"2021-06-09T08:51:15.156218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fdp.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:15.159578Z","iopub.execute_input":"2021-06-09T08:51:15.160064Z","iopub.status.idle":"2021-06-09T08:51:15.189541Z","shell.execute_reply.started":"2021-06-09T08:51:15.159993Z","shell.execute_reply":"2021-06-09T08:51:15.18831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fdp.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:15.191805Z","iopub.execute_input":"2021-06-09T08:51:15.19228Z","iopub.status.idle":"2021-06-09T08:51:15.234033Z","shell.execute_reply.started":"2021-06-09T08:51:15.192231Z","shell.execute_reply":"2021-06-09T08:51:15.233245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fdp.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:15.235129Z","iopub.execute_input":"2021-06-09T08:51:15.235521Z","iopub.status.idle":"2021-06-09T08:51:15.818428Z","shell.execute_reply.started":"2021-06-09T08:51:15.235489Z","shell.execute_reply":"2021-06-09T08:51:15.817376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fdp.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:15.820104Z","iopub.execute_input":"2021-06-09T08:51:15.820582Z","iopub.status.idle":"2021-06-09T08:51:15.82861Z","shell.execute_reply.started":"2021-06-09T08:51:15.820526Z","shell.execute_reply":"2021-06-09T08:51:15.827349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fdp.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:15.830073Z","iopub.execute_input":"2021-06-09T08:51:15.830428Z","iopub.status.idle":"2021-06-09T08:51:15.847908Z","shell.execute_reply.started":"2021-06-09T08:51:15.830396Z","shell.execute_reply":"2021-06-09T08:51:15.846706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for missing data ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\ncount_classes = pd.value_counts(data_fdp['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction class distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:15.851912Z","iopub.execute_input":"2021-06-09T08:51:15.852289Z","iopub.status.idle":"2021-06-09T08:51:16.081571Z","shell.execute_reply.started":"2021-06-09T08:51:15.85225Z","shell.execute_reply":"2021-06-09T08:51:16.080235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a highly imbalanced dataset on our hands. Normal transactions overwhelm the fraudulent ones by a large margin. Let's look at the two types of transactions: ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,7*7))\ngs = gridspec.GridSpec(7, 2)\nfor i, cn in enumerate(data_fdp.columns[:13]):\n    ax = plt.subplot(gs[i])\n    sns.histplot(data_fdp[cn][data_fdp.Class == 1], bins=50)\n    sns.histplot(data_fdp[cn][data_fdp.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:16.083675Z","iopub.execute_input":"2021-06-09T08:51:16.08419Z","iopub.status.idle":"2021-06-09T08:51:25.082972Z","shell.execute_reply.started":"2021-06-09T08:51:16.084141Z","shell.execute_reply":"2021-06-09T08:51:25.081895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat = data_fdp.corr()\nfig = plt.figure(figsize = (12, 9))\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:25.0845Z","iopub.execute_input":"2021-06-09T08:51:25.08482Z","iopub.status.idle":"2021-06-09T08:51:26.690683Z","shell.execute_reply.started":"2021-06-09T08:51:25.084788Z","shell.execute_reply":"2021-06-09T08:51:26.689894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frauds = data_fdp[data_fdp.Class == 1]\nnormal = data_fdp[data_fdp.Class == 0]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:26.691751Z","iopub.execute_input":"2021-06-09T08:51:26.692153Z","iopub.status.idle":"2021-06-09T08:51:26.745724Z","shell.execute_reply.started":"2021-06-09T08:51:26.692123Z","shell.execute_reply":"2021-06-09T08:51:26.744262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\n\nbins = 50\n\nax1.hist(frauds.Amount, bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:26.746952Z","iopub.execute_input":"2021-06-09T08:51:26.747294Z","iopub.status.idle":"2021-06-09T08:51:27.853404Z","shell.execute_reply.started":"2021-06-09T08:51:26.747261Z","shell.execute_reply":"2021-06-09T08:51:27.852175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(frauds.Time, frauds.Amount)\nax1.set_title('Fraud')\n\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:27.855082Z","iopub.execute_input":"2021-06-09T08:51:27.855505Z","iopub.status.idle":"2021-06-09T08:51:28.737451Z","shell.execute_reply.started":"2021-06-09T08:51:27.855459Z","shell.execute_reply":"2021-06-09T08:51:28.736367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Test based Time series, \nI splited the data based on the Time column, using first 75% as training/val, and last 25% as test. Since this is a very unbalanced data, the first 75% as training and validation data, and the later 25% as test, based on Time column. This is just to ensure we won’t have a test set that contains too few of positive cases (in case you want to give 90 – 10 split, for example).\n\nHence, the train_x and test_x will be our data that we will feed to model","metadata":{}},{"cell_type":"code","source":"TEST_RATIO = 0.25\ndata_fdp.sort_values('Time', inplace = True)\nTRA_INDEX = int((1-TEST_RATIO) * data_fdp.shape[0])\ntrain_x = data_fdp.iloc[:TRA_INDEX, 1:-2].values\ntrain_y = data_fdp.iloc[:TRA_INDEX, -1].values\n\ntest_x = data_fdp.iloc[TRA_INDEX:, 1:-2].values\ntest_y = data_fdp.iloc[TRA_INDEX:, -1].values","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:28.738656Z","iopub.execute_input":"2021-06-09T08:51:28.738958Z","iopub.status.idle":"2021-06-09T08:51:28.857583Z","shell.execute_reply.started":"2021-06-09T08:51:28.738922Z","shell.execute_reply":"2021-06-09T08:51:28.856456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total train examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(\n    train_x.shape[0], np.sum(train_y), np.sum(train_y)/train_x.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:28.859519Z","iopub.execute_input":"2021-06-09T08:51:28.86018Z","iopub.status.idle":"2021-06-09T08:51:28.867317Z","shell.execute_reply.started":"2021-06-09T08:51:28.860112Z","shell.execute_reply":"2021-06-09T08:51:28.866165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total test examples: {}, total fraud cases: {}, equal to {:.5f} of total cases. \".format(\n    test_x.shape[0], np.sum(test_y), np.sum(test_y)/test_y.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:28.868811Z","iopub.execute_input":"2021-06-09T08:51:28.8692Z","iopub.status.idle":"2021-06-09T08:51:28.881107Z","shell.execute_reply.started":"2021-06-09T08:51:28.869152Z","shell.execute_reply":"2021-06-09T08:51:28.880023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Standardization and Activation functions\n\nI have considered two types of standardization here – \n1. z-score \n2. min-max scaling.","metadata":{}},{"cell_type":"markdown","source":"### Feature Normalization - z score (used for tanh activation)\n\n**z-score** will normalize each column into having mean of zero and standardization of ones, which will be good choice if we are using some sort of output functions like **tanh activation function**, that outputs values on both sides of zero. Besides, this will leave values that are too extreme to still keep some extremeness left after normalization (e.g. to have more than 2 standard deviations away). This might be useful to detect outliers in this case.","metadata":{}},{"cell_type":"code","source":"cols_mean = []\ncols_std = []\nfor c in range(train_x.shape[1]):\n    cols_mean.append(train_x[:,c].mean())\n    cols_std.append(train_x[:,c].std())\n    train_x[:, c] = (train_x[:, c] - cols_mean[-1]) / cols_std[-1]\n    test_x[:, c] =  (test_x[:, c] - cols_mean[-1]) / cols_std[-1]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:28.88279Z","iopub.execute_input":"2021-06-09T08:51:28.883354Z","iopub.status.idle":"2021-06-09T08:51:28.938668Z","shell.execute_reply.started":"2021-06-09T08:51:28.883243Z","shell.execute_reply":"2021-06-09T08:51:28.937567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Normalization - min max score (used for sigmoid activation)\n\n**min-max scaling** approach will ensure all values to be within 0 - 1, all positive. This is the default approach if we are using **sigmoid activation function** as our output activation.","metadata":{}},{"cell_type":"code","source":"cols_mean = []\ncols_std = []\nfor c in range(train_x.shape[1]):\n    cols_mean.append(train_x[:,c].mean())\n    cols_std.append(train_x[:,c].std())\n    train_x[:, c] = (train_x[:, c] - cols_mean[-1]) / cols_std[-1]\n    test_x[:, c] =  (test_x[:, c] - cols_mean[-1]) / cols_std[-1]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:28.940437Z","iopub.execute_input":"2021-06-09T08:51:28.940766Z","iopub.status.idle":"2021-06-09T08:51:28.990913Z","shell.execute_reply.started":"2021-06-09T08:51:28.940732Z","shell.execute_reply":"2021-06-09T08:51:28.989888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The differences between sigmoid and tanh is shown in the image below (sigmoid will squash the values into range between (0, 1); whereas tanh, or hyperbolic tangent, squash them into (-1, 1)):\n\n![](https://vanishingcodes.files.wordpress.com/2017/06/tanh-and-sigmoid.png)\n\nI have used validation set to decide for the data standardization approach as well as activation functions. Based on related works, I found tanh to perform better than sigmoid when used together with z-score normalization. Therefore, I chose tanh followed by z-score","metadata":{}},{"cell_type":"markdown","source":"# Autoencoder\n\n\n\n## Autoencoder as unsupervised learning\n\nAutoencoder is one type of neural networks that approximates the function: **f(x) = x**. Basically, given an **input x**, the network will learn to **output f(x) i.e as close as to x**. The **error between output and input (x)** is commonly measured using **Root Mean Square Error (RMSE) – mean((f(x) – x)<sup>2</sup>)** – which is the loss function we try to minimise in our network.\n\nAn autoencoder looks like one below. It follows a typical feed-forward neural networks architecture except that the output layer has exactly same number of neurons as input layer. And it uses the input data itself as its target. Therefore it works in a way of unsupervised learning – learn without predicting an actual label.\n\nThe lower part of the network shown below is usually called an ‘encoder’ – whose job is to ’embed’ the input data into a lower dimensional array. The upper part of network, or ‘decoder’, will try to decode the embedding array into the original one.\n\nWe can have either one hidden layer, or in the case below, have multiple layers depending on the complexity of our features.\n\n![](https://vanishingcodes.files.wordpress.com/2017/06/stackedae.png)\n\n\n\n\n## Autoencoder for Fraud Detection and Prediction\n\nWe rely on autoencoder to ‘learn’ and ‘memorize’ the common patterns that are shared by the majority training data. And during reconstruction, the **RMSE** will be high for the data who do not conform to those patterns. And these are the **‘anomalies’** we are detecting. And hopefully, these ‘anomalies’ are also equal to the **‘fraudulent’** transactions we are after.\n\n### During prediction – \n1. We can select a threshold for RMSE based on validation data and flag all data with RMSE above the threshold as fraudulent. \n2. Alternatively, if we believe 0.1% of all transactions are fraudulent, we can also rank the data based on reconstruction error for each data (i.e. the RMSEs), then select the top 0.1% to be the frauds.\n\n### Evaluation metric – \nWe will evaluate our model’s performance using AUC score on test data set.","metadata":{}},{"cell_type":"code","source":"# Parameters\nlearning_rate = 0.001\ntraining_epochs = 10\nbatch_size = 256\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 15 # 1st layer num features\n#n_hidden_2 = 15 # 2nd layer num features\nn_input = train_x.shape[1] # MNIST data input (img shape: 28*28)\ndata_dir = '.'","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:28.992479Z","iopub.execute_input":"2021-06-09T08:51:28.992932Z","iopub.status.idle":"2021-06-09T08:51:28.999304Z","shell.execute_reply.started":"2021-06-09T08:51:28.992881Z","shell.execute_reply":"2021-06-09T08:51:28.998069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the Model - (1 hidden layer turned out to be enough)\n\nThe **first and second layers contain 15 and 5 neurons** respectively, we are building a network of such architecture: 28(input) -> 15 -> 5 -> 15 -> 28(output).\n\nThe activation functions for each layer used is tanh, as I explained earlier. \nThe objective function here – or the cost as above – measures the total RMSE of our predicted and input arrays in one batch – which means it’s a scalar. We then run the optimizer every time we want to do a batch update.\n\nHowever, we have another batch_mse here will return RMSEs for each input data in a batch – which is a vector of length that equals to number of rows in input data. \nThese will be the predicted values – or fraud scores if you want to call it – for the input (be it training, validation or test data), which we can extract out after prediction.","metadata":{}},{"cell_type":"code","source":"X = tf.placeholder(\"float\", [None, n_input])\n\nweights = {\n    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    #'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n    #'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n}\nbiases = {\n    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    #'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n    #'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n}\n\n\n# Building the encoder\ndef encoder(x):\n    # Encoder Hidden layer with sigmoid activation #1\n    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),\n                                   biases['encoder_b1']))\n    # Decoder Hidden layer with sigmoid activation #2\n    #layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n                                   #biases['encoder_b2']))\n    return layer_1\n\n\n# Building the decoder\ndef decoder(x):\n    # Encoder Hidden layer with sigmoid activation #1\n    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']),\n                                   biases['decoder_b1']))\n    # Decoder Hidden layer with sigmoid activation #2\n    #layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n                                  # biases['decoder_b2']))\n    return layer_1\n\n# Construct model\nencoder_op = encoder(X)\ndecoder_op = decoder(encoder_op)\n\n# Prediction\ny_pred = decoder_op\n# Targets (Labels) are the input data.\ny_true = X\n\n# Define batch mse\nbatch_mse = tf.reduce_mean(tf.pow(y_true - y_pred, 2), 1)\n\n# Define loss and optimizer, minimize the squared error\ncost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\noptimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:29.001455Z","iopub.execute_input":"2021-06-09T08:51:29.001884Z","iopub.status.idle":"2021-06-09T08:51:29.158465Z","shell.execute_reply.started":"2021-06-09T08:51:29.001837Z","shell.execute_reply":"2021-06-09T08:51:29.157388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and vallidate the Model\n\nThe training part above is straight forward. Every time we randomly sample a mini batch of size 256 from train_x, feed into model as input of X, and run the optimizer to update the parameters through SGD.\n\nHowever, one thing worth highlighting here – we are using the same data for training as well as for validation! This is reflected in the line of: **\"# Display logs per epoch step\"**\n\nThis may seem counter-intuitive in the beginning, but since we are doing unsupervised training and the model never ‘see’ the labels during training, this will not lead to overfitting. This validation process is used for monitoring **‘early stopping’ as well as model hyper-parameter tuning**. ","metadata":{}},{"cell_type":"code","source":"# TRAIN StARTS\nsave_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\nsaver = tf.train.Saver()\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    now = datetime.now()\n    sess.run(init)\n    total_batch = int(train_x.shape[0]/batch_size)\n    # Training cycle\n    for epoch in range(training_epochs):\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_idx = np.random.choice(train_x.shape[0], batch_size)\n            batch_xs = train_x[batch_idx]\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n            \n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            train_batch_mse = sess.run(batch_mse, feed_dict={X: train_x})\n            print(\"Epoch:\", '%04d' % (epoch+1),\n                  \"cost=\", \"{:.9f}\".format(c), \n                  \"Train auc=\", \"{:.6f}\".format(auc(train_y, train_batch_mse)), \n                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n\n    print(\"Optimization Finished!\")\n    \n    save_path = saver.save(sess, save_model)\n    print(\"Model saved in file: %s\" % save_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:29.159844Z","iopub.execute_input":"2021-06-09T08:51:29.160295Z","iopub.status.idle":"2021-06-09T08:51:40.758438Z","shell.execute_reply.started":"2021-06-09T08:51:29.160249Z","shell.execute_reply":"2021-06-09T08:51:40.757425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test model - on later 25% test data\n\nWe have finalized our model and hyper-parameters, we can evaluate its performance on our separate test_x data set, which is shown in codes below (test_batch_mse is our fraud scores for test data) :\n\nWe obtained an AUC of around **0.947**","metadata":{}},{"cell_type":"code","source":"save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\nsaver = tf.train.Saver()\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    now = datetime.now()\n    \n    saver.restore(sess, save_model)\n    \n    test_batch_mse = sess.run(batch_mse, feed_dict={X: test_x})\n    \n    print(\"Test AUC score: {:.6f}\".format(auc(test_y, test_batch_mse)))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:40.761131Z","iopub.execute_input":"2021-06-09T08:51:40.761457Z","iopub.status.idle":"2021-06-09T08:51:40.859978Z","shell.execute_reply.started":"2021-06-09T08:51:40.761427Z","shell.execute_reply":"2021-06-09T08:51:40.859192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the Prediction Result\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(test_y, test_batch_mse)\n\nfpr_micro, tpr_micro, _ = roc_curve(test_y, test_batch_mse)\nroc_auc = auc(fpr_micro, tpr_micro)\n\nplt.plot(fpr, tpr, color='darkorange',\n         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve on val data set')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:40.863758Z","iopub.execute_input":"2021-06-09T08:51:40.864231Z","iopub.status.idle":"2021-06-09T08:51:41.125547Z","shell.execute_reply.started":"2021-06-09T08:51:40.864197Z","shell.execute_reply":"2021-06-09T08:51:41.124476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision, recall, th = precision_recall_curve(test_y, test_batch_mse)\nplt.plot(recall, precision, 'b', label='Precision-Recall curve')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:41.127395Z","iopub.execute_input":"2021-06-09T08:51:41.127709Z","iopub.status.idle":"2021-06-09T08:51:41.352246Z","shell.execute_reply.started":"2021-06-09T08:51:41.127676Z","shell.execute_reply":"2021-06-09T08:51:41.351397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\nplt.title('Precision for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:41.35342Z","iopub.execute_input":"2021-06-09T08:51:41.353865Z","iopub.status.idle":"2021-06-09T08:51:41.590332Z","shell.execute_reply.started":"2021-06-09T08:51:41.35382Z","shell.execute_reply":"2021-06-09T08:51:41.589049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\nplt.title('Recall for different threshold values')\nplt.xlabel('Reconstruction error')\nplt.ylabel('Recall')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:41.591832Z","iopub.execute_input":"2021-06-09T08:51:41.59218Z","iopub.status.idle":"2021-06-09T08:51:41.838633Z","shell.execute_reply.started":"2021-06-09T08:51:41.592145Z","shell.execute_reply":"2021-06-09T08:51:41.837797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.hist(test_cost[np.where(test_y == 1)], bins = 100)\nplt.title('Fraud score (mse) distribution of val set')\nplt.xlabel('Fraud score')\nplt.ylabel('Probabilties')\nplt.hist(test_batch_mse[(test_y == 0) & (test_batch_mse < 1000)], bins = 100, color='green', density=True, label='Non-Fraud')\nplt.hist(test_batch_mse[(test_y == 1) & (test_batch_mse < 1000)], bins = 100, color='red', density=True, label = 'Fraud')\n\nplt.legend(loc=\"upper right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:41.839816Z","iopub.execute_input":"2021-06-09T08:51:41.840154Z","iopub.status.idle":"2021-06-09T08:51:42.512513Z","shell.execute_reply.started":"2021-06-09T08:51:41.840121Z","shell.execute_reply":"2021-06-09T08:51:42.511421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Display fraud score (mse) distribution for Fraud cases","metadata":{}},{"cell_type":"code","source":"#plt.hist(test_cost[np.where(test_y == 1)], bins = 100)\nplt.hist(test_batch_mse[(test_y == 1) & (test_batch_mse < 500)], bins = 100, color='red', density=True)\nplt.title('Fraud score (mse) distribution of Fraud cases')\nplt.xlabel('Fraud score')\nplt.ylabel('Probabilities')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:42.514215Z","iopub.execute_input":"2021-06-09T08:51:42.51483Z","iopub.status.idle":"2021-06-09T08:51:42.959593Z","shell.execute_reply.started":"2021-06-09T08:51:42.514781Z","shell.execute_reply":"2021-06-09T08:51:42.958306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zoom into (0, 20) range","metadata":{}},{"cell_type":"markdown","source":"#### 2. Display fraud score (mse) distribution for Non-fraud cases","metadata":{}},{"cell_type":"code","source":"#plt.hist(test_cost[np.where(test_y == 0)], bins = 100)\nplt.hist(test_batch_mse[(test_y == 0) & (test_batch_mse < 500)], bins = 100, color='green', density=True)\nplt.title('Fraud score (mse) distribution of non-fraud cases')\nplt.xlabel('Fraud score')\nplt.ylabel('Probabilities')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:42.96128Z","iopub.execute_input":"2021-06-09T08:51:42.961658Z","iopub.status.idle":"2021-06-09T08:51:43.386945Z","shell.execute_reply.started":"2021-06-09T08:51:42.961625Z","shell.execute_reply":"2021-06-09T08:51:43.385846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Threshold Test\n\nThreshold Test value = 7\n\n### Observation \n\nOur precision increased by a factor of 60 from 0.132% to 7.86%; However, the detection precision is still low (below 8%), but this is mainly due to the overall percentage of fraud cases is really too low. ","metadata":{}},{"cell_type":"code","source":"THRE_TEST = 7\nprint(\"Let's, for example, use 7 as our detection threshold: \\n\\\nNumber of detected cases above treshold: {}, \\n\\\nNumber of pos cases only above threshold: {}, \\n\\\nThe percentage of accuracy above treshold (Precision): {:0.2f}%. \\n\\\nCompared to the average percentage of fraud in test set: 0.132%\".format( \\\nnp.sum(test_batch_mse > THRE_TEST), \\\nnp.sum(test_y[test_batch_mse > THRE_TEST]), \\\nnp.sum(test_y[test_batch_mse > THRE_TEST]) / np.sum(test_batch_mse > THRE_TEST) * 100))\n      ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:43.388574Z","iopub.execute_input":"2021-06-09T08:51:43.388987Z","iopub.status.idle":"2021-06-09T08:51:43.396734Z","shell.execute_reply.started":"2021-06-09T08:51:43.388942Z","shell.execute_reply":"2021-06-09T08:51:43.395636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a Binary Classifier that Predicts Fraud \n\nUsing the Autoencoder embedding layers as Model Inputs, we will build a Fully Connected(FC) Feedforward Neural Network Model \n\nUntil now we have covered all the necessary steps to train an autoencoder and make predictions on test data. However, the next thing I will do is ‘pre-train’ our data set from scratch using autoencoder, fetch out the embedding layers, and feed those embeddings to a FC Feedforward Neural Network that will do the task of binary classification.\n\nThe rationale is simple – since our autoencoder is able to differentiate between frauds and non-frauds, the lower dimensional features it’s derived (the embedding layer) during training should include some useful latent features that would help the task of fraud classification, or at least, it should speed up classifier’s learning process, compared to letting it adapt to the raw features from scratch.\n\nFirst, let’s fetch out the embedding layer for all data set(i.e both train and test data). It is the **encoder_op**. Tensor ops which we can get by using **sess.run**.","metadata":{}},{"cell_type":"code","source":"save_model = os.path.join(data_dir, 'temp_saved_model_1layer.ckpt')\nsaver = tf.train.Saver()\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    now = datetime.now()\n    saver.restore(sess, save_model)\n    \n    test_encoding = sess.run(encoder_op, feed_dict={X: test_x})\n    train_encoding = sess.run(encoder_op, feed_dict={X: train_x})\n    \n    print(\"Dim for test_encoding and train_encoding are: \\n\", test_encoding.shape, '\\n', train_encoding.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:43.398632Z","iopub.execute_input":"2021-06-09T08:51:43.39907Z","iopub.status.idle":"2021-06-09T08:51:43.513293Z","shell.execute_reply.started":"2021-06-09T08:51:43.399023Z","shell.execute_reply":"2021-06-09T08:51:43.512204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the graph for FC layers \n\nSecond, we build the graph for FC feed-forward neural network as follows (again, you could use validation to fine tune hyper-parameters such as hidden layer numbers and sizes):\n\nBest hidden size based on validation is found to be 4","metadata":{}},{"cell_type":"code","source":"#n_input = test_encoding.shape[1]\nn_input = test_encoding.shape[1]\n\nhidden_size = 4\noutput_size = 2\n\nX = tf.placeholder(tf.float32, [None, n_input], name='input_x')\ny_ = tf.placeholder(tf.int32, shape=[None, output_size], name='target_y')\n\nweights = {\n    'W1': tf.Variable(tf.truncated_normal([n_input, hidden_size])),\n    'W2': tf.Variable(tf.truncated_normal([hidden_size, output_size])),\n}\nbiases = {\n    'b1': tf.Variable(tf.zeros([hidden_size])),\n    'b2': tf.Variable(tf.zeros([output_size])),\n}\n\nhidden_layer =  tf.nn.relu(tf.add(tf.matmul(X, weights['W1']), biases['b1']))\npred_logits = tf.add(tf.matmul(hidden_layer, weights['W2']), biases['b2'])\npred_probs = tf.nn.softmax(pred_logits)\n\ncross_entropy = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=pred_logits))\n\noptimizer = tf.train.AdamOptimizer(2e-4).minimize(cross_entropy)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:43.515299Z","iopub.execute_input":"2021-06-09T08:51:43.515605Z","iopub.status.idle":"2021-06-09T08:51:43.647824Z","shell.execute_reply.started":"2021-06-09T08:51:43.515574Z","shell.execute_reply":"2021-06-09T08:51:43.646746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare the data set. \n\nWe will prepare our data set, and train the model while monitoring its validation scores. \nHere, we will further split our **train_encoding** into **train_enc_x** and **val_enc_x**, each taking up **80**% and **20**% of the previous train_encoding , respectively. \nAs a typical supervised training approach, we will use **train_enc_x** as our **training data** and **val_enc_x** as **validation data**.\n\nWe will therefore use 80% of out previous training data as our new training, and the remaining 20% as new validation.\n","metadata":{}},{"cell_type":"code","source":"n_epochs = 80\nbatch_size = 256\n\n# PREPARE DATA\nVAL_PERC = 0.2\nall_y_bin = np.zeros((data_fdp.shape[0], 2))\nall_y_bin[range(data_fdp.shape[0]), data_fdp['Class'].values] = 1\n\ntrain_enc_x = train_encoding[:int(train_encoding.shape[0] * (1-VAL_PERC))]\ntrain_enc_y = all_y_bin[:int(train_encoding.shape[0] * (1-VAL_PERC))]\n\nval_enc_x = train_encoding[int(train_encoding.shape[0] * (1-VAL_PERC)):]\nval_enc_y = all_y_bin[int(train_encoding.shape[0] * (1-VAL_PERC)):train_encoding.shape[0]]\n\ntest_enc_y = all_y_bin[train_encoding.shape[0]:]\nprint(\"Num of data for train, val and test are: \\n{}, \\n{}, \\n{}\".format(train_enc_x.shape[0], val_enc_x.shape[0], \\\n                                                                        test_encoding.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:43.649356Z","iopub.execute_input":"2021-06-09T08:51:43.649822Z","iopub.status.idle":"2021-06-09T08:51:43.754292Z","shell.execute_reply.started":"2021-06-09T08:51:43.649774Z","shell.execute_reply":"2021-06-09T08:51:43.753194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Binary Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score as auc \n\n# TRAIN STARTS\nsave_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\nsaver = tf.train.Saver()\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    now = datetime.now()\n    sess.run(init)\n    total_batch = int(train_enc_x.shape[0]/batch_size)\n    # Training cycle\n    for epoch in range(n_epochs):\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_idx = np.random.choice(train_enc_x.shape[0], batch_size)\n            batch_xs = train_enc_x[batch_idx]\n            batch_ys = train_enc_y[batch_idx]\n\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys})\n            \n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            val_probs = sess.run(pred_probs, feed_dict={X: val_enc_x})\n            print(\"Epoch:\", '%04d' % (epoch+1),\n                  \"cost=\", \"{:.9f}\".format(c), \n                  \"Val auc=\", \"{:.6f}\".format(auc(val_enc_y[:, 1], val_probs[:, 1])), \n                  \"Time elapsed=\", \"{}\".format(datetime.now() - now))\n\n    print(\"Optimization Finished!\")\n    \n    save_path = saver.save(sess, save_model)\n    print(\"Model saved in file: %s\" % save_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:51:43.755821Z","iopub.execute_input":"2021-06-09T08:51:43.756449Z","iopub.status.idle":"2021-06-09T08:52:33.080306Z","shell.execute_reply.started":"2021-06-09T08:51:43.756401Z","shell.execute_reply":"2021-06-09T08:52:33.079178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test the Model \n\nWe have finalized our model, now we will evaluate its performance on the same test data as before,\n\nWe obtained a slightly improved AUC of around **0.96**","metadata":{}},{"cell_type":"code","source":"save_model = os.path.join(data_dir, 'temp_saved_model_FCLayers.ckpt')\nsaver = tf.train.Saver()\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    now = datetime.now()\n    \n    saver.restore(sess, save_model)\n    \n    test_probs = sess.run(pred_probs, feed_dict={X: test_encoding})\n    \n    print(\"\\nTest auc score: {}\".format(auc(test_enc_y[:, 1], test_probs[:, 1])))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:33.08727Z","iopub.execute_input":"2021-06-09T08:52:33.087655Z","iopub.status.idle":"2021-06-09T08:52:33.194057Z","shell.execute_reply.started":"2021-06-09T08:52:33.087622Z","shell.execute_reply":"2021-06-09T08:52:33.192819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the Prediction Result\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(test_enc_y[:, 1], test_probs[:, 1])\n\nfpr_micro, tpr_micro, _ = roc_curve(test_enc_y[:, 1], test_probs[:, 1])\nroc_auc = auc(fpr_micro, tpr_micro)\n\nplt.plot(fpr, tpr, color='darkorange',\n         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve on val data set')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:33.19599Z","iopub.execute_input":"2021-06-09T08:52:33.196427Z","iopub.status.idle":"2021-06-09T08:52:33.468253Z","shell.execute_reply.started":"2021-06-09T08:52:33.196361Z","shell.execute_reply":"2021-06-09T08:52:33.467038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision, recall, th = precision_recall_curve(test_y, test_batch_mse)\nplt.plot(recall, precision, 'b', label='Precision-Recall curve')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:33.470039Z","iopub.execute_input":"2021-06-09T08:52:33.470364Z","iopub.status.idle":"2021-06-09T08:52:34.148887Z","shell.execute_reply.started":"2021-06-09T08:52:33.470333Z","shell.execute_reply":"2021-06-09T08:52:34.147728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\nplt.title('Precision for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:34.150416Z","iopub.execute_input":"2021-06-09T08:52:34.150738Z","iopub.status.idle":"2021-06-09T08:52:34.387589Z","shell.execute_reply.started":"2021-06-09T08:52:34.150705Z","shell.execute_reply":"2021-06-09T08:52:34.386752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\nplt.title('Recall for different threshold values')\nplt.xlabel('Reconstruction error')\nplt.ylabel('Recall')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:34.388664Z","iopub.execute_input":"2021-06-09T08:52:34.389071Z","iopub.status.idle":"2021-06-09T08:52:34.632261Z","shell.execute_reply.started":"2021-06-09T08:52:34.389026Z","shell.execute_reply":"2021-06-09T08:52:34.631165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.hist(test_cost[np.where(test_y == 1)], bins = 100)\nplt.title('Fraud score(mse) distribution of val set')\nplt.xlabel('Fraud score')\nplt.ylabel('Probabilties')\nplt.hist(test_probs[(test_enc_y == 0) & (test_probs < 500)], bins = 100, color='green', density=True, label='Non-Fraud')\nplt.hist(test_probs[(test_enc_y == 1) & (test_probs < 500)], bins = 100, color='red', density=True, label = 'Fraud')\n\nplt.legend(loc=\"upper right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:34.633494Z","iopub.execute_input":"2021-06-09T08:52:34.633916Z","iopub.status.idle":"2021-06-09T08:52:35.231104Z","shell.execute_reply.started":"2021-06-09T08:52:34.633882Z","shell.execute_reply":"2021-06-09T08:52:35.230372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Display fraud score (mse) distribution for non-fraud cases","metadata":{}},{"cell_type":"code","source":"#plt.hist(test_cost[np.where(test_y == 1)], bins = 100)\nplt.hist(test_probs[(test_enc_y == 1) & (test_probs < 500)], bins = 100, color='red', density=True)\nplt.title('Fraud score (mse) distribution of Fraud cases')\nplt.xlabel('Fraud score')\nplt.ylabel('Probabilities')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:35.232147Z","iopub.execute_input":"2021-06-09T08:52:35.232532Z","iopub.status.idle":"2021-06-09T08:52:35.627232Z","shell.execute_reply.started":"2021-06-09T08:52:35.232502Z","shell.execute_reply":"2021-06-09T08:52:35.62644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zoom into (0, 20) range","metadata":{}},{"cell_type":"markdown","source":"#### 2. Display fraud score (mse) distribution for fraud cases","metadata":{}},{"cell_type":"code","source":"#plt.hist(test_cost[np.where(test_y == 1)], bins = 100)\nplt.hist(test_probs[(test_enc_y == 0) & (test_probs < 500)], bins = 100, color='green', density=True)\nplt.title('Fraud score (mse) distribution of Non-fraud cases')\nplt.xlabel('Fraud score')\nplt.ylabel('Probabilities')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:52:35.628303Z","iopub.execute_input":"2021-06-09T08:52:35.62869Z","iopub.status.idle":"2021-06-09T08:52:36.035897Z","shell.execute_reply.started":"2021-06-09T08:52:35.62866Z","shell.execute_reply":"2021-06-09T08:52:36.03508Z"},"trusted":true},"execution_count":null,"outputs":[]}]}